{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2849c7-7e88-4bc4-b6dc-9ece72aaac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-1 part-1 understandings regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e572295-7fea-41b3-85bb-17ab22830f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization in Deep Learning:\n",
    "Definition:\n",
    "Regularization is a technique used in machine learning and deep learning to prevent overfitting by adding a penalty term to the loss function. The penalty discourages the model from fitting the training data too closely and helps in generalizing better to unseen data.\n",
    "\n",
    "Importance:\n",
    "In deep learning, models with a large number of parameters can be prone to overfitting, where they memorize the training data instead of learning the underlying patterns. Regularization helps in controlling the model complexity, reducing the risk of overfitting, and improving its ability to generalize well on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25446ac4-ed3a-48b6-aca2-ff99e76313dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6079244c-1eef-47d8-9db1-4db5aa970643",
   "metadata": {},
   "outputs": [],
   "source": [
    "The bias-variance tradeoff is a key concept in machine learning that illustrates the balance between bias and variance in a model. Here's a breakdown:\n",
    "\n",
    "Bias: Bias refers to the error introduced by approximating a real-world problem with a simplified model. High bias can lead to underfitting, where the model is too simplistic and cannot capture the underlying patterns in the data.\n",
    "\n",
    "Variance: Variance is the error introduced by using a complex model that is highly sensitive to the training data. High variance can lead to overfitting, where the model fits the training data too closely, capturing noise and not generalizing well to new, unseen data.\n",
    "\n",
    "Bias-Variance Tradeoff:\n",
    "\n",
    "A model with high bias has low variance, and vice versa.\n",
    "There is a tradeoff between bias and variance – as one decreases, the other increases.\n",
    "The goal is to find the optimal balance that minimizes both bias and variance, resulting in a model that generalizes well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b079f1-1c8e-4289-8738-67ed4285ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba4a5fb-c6d7-45a5-b19c-2461717c3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 and L2 Regularization:\n",
    "L1 Regularization (Lasso):\n",
    "\n",
    "Penalty Calculation: Adds the sum of absolute values of the weights to the loss function: \n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "∣\n",
    "�\n",
    "�\n",
    "∣\n",
    "λ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " ∣w \n",
    "i\n",
    "​\n",
    " ∣.\n",
    "Effect: Encourages sparsity in the weight matrix, leading to some weights becoming exactly zero. It is useful for feature selection.\n",
    "L2 Regularization (Ridge):\n",
    "\n",
    "Penalty Calculation: Adds the sum of squared values of the weights to the loss function: \n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "2\n",
    "λ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " w \n",
    "i\n",
    "2\n",
    "​\n",
    " .\n",
    "Effect: Discourages extreme values in the weight matrix, preventing any single weight from becoming too dominant.\n",
    "Differences:\n",
    "\n",
    "Calculation: L1 regularization uses the absolute values of the weights, while L2 uses the squared values.\n",
    "\n",
    "Sparsity: L1 regularization tends to produce sparse weight matrices (some weights become exactly zero), making it useful for feature selection. L2 regularization penalizes extreme values but generally keeps all weights non-zero.\n",
    "\n",
    "Robustness: L1 regularization can be more robust in the presence of irrelevant features, as it tends to ignore them by setting corresponding weights to zero. L2 regularization provides more evenly distributed, smaller weights.\n",
    "\n",
    "Effect on Bias-Variance Tradeoff:\n",
    "\n",
    "Both L1 and L2 regularization contribute to addressing the bias-variance tradeoff by adding a penalty term to the loss function.\n",
    "\n",
    "They help control the model complexity, preventing overfitting (high variance) by discouraging overly complex models.\n",
    "\n",
    "In summary, L1 and L2 regularization are techniques that play a crucial role in finding the right balance between bias and variance, contributing to the overall generalization performance of a machine learning model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a51949-b613-4c4b-afb7-9b5b86b4ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed6913e-8ef5-4038-9cf0-17c7e4841ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization is a crucial technique in deep learning for preventing overfitting and improving the generalization of models. Here are key aspects of its role:\n",
    "\n",
    "1. Controlling Model Complexity:\n",
    "Regularization adds a penalty term to the loss function based on the complexity of the model. This penalty discourages the use of overly complex models that may fit the training data too closely.\n",
    "By controlling model complexity, regularization helps prevent overfitting, where the model memorizes noise in the training data instead of learning general patterns.\n",
    "2. Bias-Variance Tradeoff:\n",
    "Regularization contributes to finding the right balance between bias and variance, known as the bias-variance tradeoff.\n",
    "It helps avoid models that are too simple (high bias) or too complex (high variance), promoting models that generalize well to new, unseen data.\n",
    "3. L1 and L2 Regularization Effects:\n",
    "L1 (Lasso) Regularization: Encourages sparsity in the weight matrix, promoting some weights to become exactly zero. This aids in feature selection and simplifies the model.\n",
    "L2 (Ridge) Regularization: Discourages extreme values in the weight matrix, preventing any single weight from dominating. It provides a smoother distribution of weights.\n",
    "4. Preventing Over-Reliance on Specific Features:\n",
    "Regularization helps prevent the over-reliance on individual features by penalizing large weights. This encourages the model to use a more diverse set of features, improving its robustness.\n",
    "5. Improving Robustness to Noise:\n",
    "The penalty term introduced by regularization makes the model less sensitive to small variations and noise in the training data.\n",
    "This improved robustness enhances the model's ability to generalize to real-world scenarios where data may have inherent variability.\n",
    "6. Encouraging Simplicity:\n",
    "Regularization encourages models to be simple, favoring solutions with fewer parameters. This simplicity often leads to more interpretable and understandable models.\n",
    "7. Cross-Validation and Hyperparameter Tuning:\n",
    "Regularization parameters, such as the strength of the penalty term, are often tuned using cross-validation techniques.\n",
    "Proper hyperparameter tuning ensures that regularization is effective in preventing overfitting and improving generalization.\n",
    "8. Early Stopping:\n",
    "Early stopping, which involves halting the training process when the model's performance on a validation set starts to degrade, can be considered a form of regularization.\n",
    "It prevents the model from fitting noise in the training data by stopping training before overfitting occurs.\n",
    "In summary, regularization plays a pivotal role in deep learning by providing a mechanism to control model complexity, address the bias-variance tradeoff, and enhance the generalization capability of models, making them more robust and applicable to diverse datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60c03a9-1623-4ca6-8164-66c69b45acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-5 part-2 regulerization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1093c-d80e-40a4-9f8c-3a75e46f7c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout is a regularization technique commonly used in neural networks to reduce overfitting. It involves randomly \"dropping out\" (setting to zero) a fraction of neurons during each training iteration. This means that for each forward and backward pass, a random subset of neurons is ignored.\n",
    "\n",
    "How Dropout Works:\n",
    "\n",
    "Training Phase:\n",
    "\n",
    "During training, for each mini-batch of data, a random subset of neurons is selected to be \"dropped out\" with a probability \n",
    "�\n",
    "p. This probability is a hyperparameter typically set between 0.2 and 0.5.\n",
    "The dropped-out neurons do not contribute to the forward or backward pass during that iteration.\n",
    "Inference Phase:\n",
    "\n",
    "During inference or model evaluation, all neurons are used. However, their weights are scaled by a factor equal to \n",
    "1\n",
    "−\n",
    "�\n",
    "1−p to compensate for the fact that more neurons were active during training.\n",
    "Impact on Model Training:\n",
    "\n",
    "Reduction in Overfitting:\n",
    "\n",
    "Dropout helps prevent co-adaptation of neurons, where certain neurons become overly dependent on each other.\n",
    "It introduces robustness by ensuring that no single neuron or group of neurons becomes too specialized, making the model less prone to overfitting.\n",
    "Ensemble Effect:\n",
    "\n",
    "Each dropout configuration during training can be viewed as training a different neural network. Dropout effectively creates an ensemble of multiple subnetworks.\n",
    "The ensemble effect improves generalization by combining the knowledge from various subnetworks, capturing different aspects of the data.\n",
    "Promoting Robust Features:\n",
    "\n",
    "Dropout forces the network to rely on a more diverse set of features, as different neurons are dropped out in different iterations.\n",
    "This encourages the model to learn more robust and generalizable features.\n",
    "Impact on Model Inference:\n",
    "\n",
    "Scaling Weights:\n",
    "\n",
    "During inference, all neurons are active, but their weights are scaled by a factor \n",
    "1\n",
    "−\n",
    "�\n",
    "1−p to account for the fact that more neurons were active during training.\n",
    "This scaling ensures that the expected value of each neuron's output remains the same between training and inference.\n",
    "No Dropout during Inference:\n",
    "\n",
    "Dropout is not applied during inference because the goal is to use the entire trained network for making predictions without the random dropout mechanism.\n",
    "Considerations:\n",
    "\n",
    "Dropout may increase training time because multiple subnetworks are effectively trained in parallel during each mini-batch.\n",
    "It is not typically used in conjunction with batch normalization, as both techniques have a similar effect of adding noise to the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb53600b-449b-407f-a0bf-6a06f6410aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b9a9b5-e835-4264-936b-6b9ee158649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Early Stopping as a Form of Regularization:\n",
    "Concept:\n",
    "Early stopping is a regularization technique used in machine learning, including deep learning, to prevent overfitting during the training process. Instead of training the model until it perfectly fits the training data, early stopping involves monitoring the model's performance on a validation dataset and stopping the training process when the performance starts to degrade.\n",
    "\n",
    "How It Works:\n",
    "\n",
    "Training Monitoring:\n",
    "\n",
    "During training, the model's performance is evaluated on a separate validation dataset at regular intervals (after each epoch or a fixed number of iterations).\n",
    "Detection of Performance Degradation:\n",
    "\n",
    "The training process is halted if the performance on the validation set ceases to improve or starts to worsen. The criterion for degradation could be based on metrics like validation loss or accuracy.\n",
    "Model Snapshot:\n",
    "\n",
    "The model parameters at the point of early stopping (when performance was optimal on the validation set) are saved as the final model.\n",
    "Benefits and How It Prevents Overfitting:\n",
    "\n",
    "Preventing Overfitting:\n",
    "\n",
    "As the model continues to train, it may start to memorize the noise in the training data (overfitting). Early stopping prevents the model from becoming overly complex and overfitting by stopping the training process when the model's performance on new, unseen data (validation set) begins to degrade.\n",
    "Finding the Optimal Point:\n",
    "\n",
    "Early stopping helps find a balance between training the model long enough to capture essential patterns and stopping before it starts fitting noise.\n",
    "It avoids training to the point where the model's performance on the validation set begins to suffer due to overfitting.\n",
    "Generalization Improvement:\n",
    "\n",
    "By selecting the model at the point of early stopping, which corresponds to optimal validation performance, it is more likely to generalize well to new, unseen data.\n",
    "Resource Efficiency:\n",
    "\n",
    "Early stopping can save computational resources by preventing unnecessary iterations. Training deep learning models can be computationally intensive, and stopping early once optimal performance is reached can be resource-efficient.\n",
    "Considerations:\n",
    "\n",
    "The choice of the performance metric for early stopping is crucial and depends on the specific task (e.g., validation loss, accuracy, etc.).\n",
    "Early stopping is often combined with other regularization techniques, such as dropout or weight decay, for a more comprehensive approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e243688-569e-409b-8488-ac977adef057",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebdb0ec-a260-4660-9b3e-96d13f98f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch Normalization and its Role as Regularization:\n",
    "Concept of Batch Normalization (BN):\n",
    "Batch Normalization is a technique used in deep learning to address issues related to internal covariate shift and accelerate the training process. It involves normalizing the inputs of each layer in a neural network by adjusting and scaling the activations.\n",
    "\n",
    "Key Steps of Batch Normalization:\n",
    "\n",
    "Normalization:\n",
    "\n",
    "For each mini-batch during training, BN normalizes the inputs by subtracting the mean and dividing by the standard deviation. This centers and scales the inputs, making them have a mean of zero and a standard deviation of one.\n",
    "Scaling and Shifting:\n",
    "\n",
    "After normalization, the normalized inputs are scaled by a learnable parameter (gamma) and shifted by another learnable parameter (beta).\n",
    "This introduces flexibility, allowing the network to learn the optimal scaling and shifting for each feature.\n",
    "Applicability:\n",
    "\n",
    "Batch Normalization is typically applied before the activation function in a neural network.\n",
    "Role of Batch Normalization as Regularization:\n",
    "Batch Normalization has inherent regularization effects that contribute to preventing overfitting:\n",
    "\n",
    "Reducing Internal Covariate Shift:\n",
    "\n",
    "Internal covariate shift occurs when the distribution of the inputs to a layer changes during training. BN mitigates this by normalizing inputs, making the learning process more stable.\n",
    "Reducing Dependency on Specific Weights:\n",
    "\n",
    "BN reduces the dependence of a layer on the specific parameters of the previous layers. This helps prevent overfitting by making the network more robust to variations in weights.\n",
    "Introducing Noise:\n",
    "\n",
    "During training, BN introduces a certain level of noise to the activations because the normalization is performed on mini-batches. This acts as a form of regularization, similar to dropout, making the network less prone to overfitting.\n",
    "Allowing Higher Learning Rates:\n",
    "\n",
    "BN allows the use of higher learning rates during training, which can speed up convergence. Faster convergence often leads to models that generalize better.\n",
    "Smooth Gradient Flow:\n",
    "\n",
    "BN tends to maintain a more consistent distribution of activations throughout the network, leading to smoother gradients during backpropagation. This can help prevent exploding or vanishing gradients, contributing to more stable training.\n",
    "Considerations:\n",
    "\n",
    "Batch Normalization is effective in preventing overfitting, but it is not a replacement for other regularization techniques like dropout.\n",
    "During inference, BN utilizes the estimated mean and standard deviation from the training phase, ensuring consistent normalization.\n",
    "In summary, Batch Normalization acts as a regularization technique by stabilizing and normalizing activations during training, reducing internal covariate shift, and introducing noise. These effects contribute to making the model more robust and preventing overfitting in deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c0bc71-480a-42a4-8446-0aeef922c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-8 part-3 applying regulerization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3412821e-0aba-4288-a9a8-181041869824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0.post1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (3.7.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (1.14.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (0.5.4)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (16.0.6)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (23.5.26)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (2.15.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (2.15.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (65.5.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (2.15.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (1.23.5)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (2.4.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (2.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (1.60.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (0.2.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (22.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (0.35.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (4.21.11)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from Tensorflow) (4.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->Tensorflow) (0.38.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->Tensorflow) (3.5.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->Tensorflow) (1.2.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->Tensorflow) (2.25.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->Tensorflow) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->Tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->Tensorflow) (3.0.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->Tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->Tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->Tensorflow) (0.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->Tensorflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->Tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->Tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->Tensorflow) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->Tensorflow) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->Tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->Tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->Tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b569ad-be3c-4403-b2ad-938e55be0fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 12:57:16.450375: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-21 12:57:16.450503: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-21 12:57:16.581051: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-21 12:57:16.831484: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-21 12:57:18.681036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "844/844 [==============================] - 4s 4ms/step - loss: 0.3185 - accuracy: 0.9112 - val_loss: 0.1795 - val_accuracy: 0.9497\n",
      "Epoch 2/10\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.1437 - accuracy: 0.9585 - val_loss: 0.1201 - val_accuracy: 0.9653\n",
      "Epoch 3/10\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.1003 - accuracy: 0.9704 - val_loss: 0.0982 - val_accuracy: 0.9725\n",
      "Epoch 4/10\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.0769 - accuracy: 0.9773 - val_loss: 0.0940 - val_accuracy: 0.9722\n",
      "Epoch 5/10\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.0600 - accuracy: 0.9826 - val_loss: 0.0849 - val_accuracy: 0.9728\n",
      "Epoch 6/10\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.0494 - accuracy: 0.9854 - val_loss: 0.0754 - val_accuracy: 0.9770\n",
      "Epoch 7/10\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.0393 - accuracy: 0.9887 - val_loss: 0.0716 - val_accuracy: 0.9775\n",
      "Epoch 8/10\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.0325 - accuracy: 0.9899 - val_loss: 0.0772 - val_accuracy: 0.9772\n",
      "Epoch 9/10\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.0268 - accuracy: 0.9923 - val_loss: 0.0726 - val_accuracy: 0.9787\n",
      "Epoch 10/10\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.0233 - accuracy: 0.9932 - val_loss: 0.0747 - val_accuracy: 0.9783\n",
      "Epoch 1/10\n",
      "844/844 [==============================] - 4s 4ms/step - loss: 0.4582 - accuracy: 0.8657 - val_loss: 0.1971 - val_accuracy: 0.9427\n",
      "Epoch 2/10\n",
      "844/844 [==============================] - 3s 4ms/step - loss: 0.2483 - accuracy: 0.9269 - val_loss: 0.1442 - val_accuracy: 0.9573\n",
      "Epoch 3/10\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.2074 - accuracy: 0.9390 - val_loss: 0.1210 - val_accuracy: 0.9668\n",
      "Epoch 4/10\n",
      "844/844 [==============================] - 3s 4ms/step - loss: 0.1809 - accuracy: 0.9475 - val_loss: 0.1101 - val_accuracy: 0.9685\n",
      "Epoch 5/10\n",
      "844/844 [==============================] - 3s 4ms/step - loss: 0.1636 - accuracy: 0.9518 - val_loss: 0.0986 - val_accuracy: 0.9710\n",
      "Epoch 6/10\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.1489 - accuracy: 0.9552 - val_loss: 0.0931 - val_accuracy: 0.9732\n",
      "Epoch 7/10\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.1425 - accuracy: 0.9570 - val_loss: 0.0910 - val_accuracy: 0.9720\n",
      "Epoch 8/10\n",
      "844/844 [==============================] - 3s 4ms/step - loss: 0.1307 - accuracy: 0.9593 - val_loss: 0.0836 - val_accuracy: 0.9768\n",
      "Epoch 9/10\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.1280 - accuracy: 0.9605 - val_loss: 0.0814 - val_accuracy: 0.9755\n",
      "Epoch 10/10\n",
      "844/844 [==============================] - 3s 3ms/step - loss: 0.1204 - accuracy: 0.9619 - val_loss: 0.0783 - val_accuracy: 0.9773\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Accuracy without Dropout: 0.9764\n",
      "Accuracy with Dropout: 0.9758\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values to be between 0 and 1\n",
    "\n",
    "# Flatten the images\n",
    "x_train = x_train.reshape((x_train.shape[0], -1))\n",
    "x_test = x_test.reshape((x_test.shape[0], -1))\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Function to create a simple feedforward neural network model\n",
    "def create_model(use_dropout):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(784,)))\n",
    "    \n",
    "    # Add Dropout layer if specified\n",
    "    if use_dropout:\n",
    "        model.add(Dropout(0.5))  # You can adjust the dropout rate as needed\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "# Create models with and without Dropout\n",
    "model_without_dropout = create_model(use_dropout=False)\n",
    "model_with_dropout = create_model(use_dropout=True)\n",
    "\n",
    "# Compile both models\n",
    "model_without_dropout.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_with_dropout.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train both models\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "history_without_dropout = model_without_dropout.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val))\n",
    "history_with_dropout = model_with_dropout.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val))\n",
    "\n",
    "# Evaluate both models on the test set\n",
    "y_pred_without_dropout = np.argmax(model_without_dropout.predict(x_test), axis=1)\n",
    "y_pred_with_dropout = np.argmax(model_with_dropout.predict(x_test), axis=1)\n",
    "\n",
    "accuracy_without_dropout = accuracy_score(y_test, y_pred_without_dropout)\n",
    "accuracy_with_dropout = accuracy_score(y_test, y_pred_with_dropout)\n",
    "\n",
    "print(\"Accuracy without Dropout:\", accuracy_without_dropout)\n",
    "print(\"Accuracy with Dropout:\", accuracy_with_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5b15d14-aa64-441a-929f-cf42856b30cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f199bf-31fc-45bc-afdb-d547e3932c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Choosing the appropriate regularization technique for a deep learning task involves considering various factors and trade-offs. Here are some key considerations:\n",
    "\n",
    "1. Nature of the Data:\n",
    "Sparse or Dense Data: L1 regularization (Lasso) tends to work well with sparse data because it encourages sparsity by setting some weights to exactly zero. In contrast, L2 regularization (Ridge) works well with dense data.\n",
    "Feature Relationships: Understanding the relationships between features can guide the choice of regularization. For example, if features are highly correlated, L2 regularization might be more suitable.\n",
    "2. Model Complexity:\n",
    "Model Size: In larger models, regularization becomes more critical to prevent overfitting. Techniques like dropout and batch normalization are effective for large networks.\n",
    "Architecture: Certain architectures may benefit more from specific regularization techniques. For example, recurrent neural networks (RNNs) might benefit from dropout between recurrent layers.\n",
    "3. Computational Resources:\n",
    "Training Time: Regularization techniques introduce additional computational overhead, especially during training. Techniques like dropout can increase training time due to the random dropout of neurons.\n",
    "Memory Usage: Some regularization techniques, like batch normalization, require additional memory to store the mean and variance of each feature.\n",
    "4. Interpretability:\n",
    "Interpretability of Features: L1 regularization can promote sparsity, making the model more interpretable by highlighting a subset of important features. L2 regularization tends to distribute weights more evenly.\n",
    "Understanding Dropout: Dropout can make it challenging to interpret individual neurons' importance, as the network learns to be robust by using different subsets of neurons.\n",
    "5. Performance Metrics:\n",
    "Task-specific Metrics: The choice of regularization may depend on the performance metric relevant to the task. For example, if false positives are more costly, emphasizing precision might be crucial.\n",
    "6. Robustness and Generalization:\n",
    "Desired Robustness: Techniques like dropout and batch normalization introduce noise during training, making the model more robust. Understanding the desired level of robustness is crucial.\n",
    "Generalization: Regularization's primary goal is to improve the model's generalization to new, unseen data. Consider the expected distribution shift between training and test data.\n",
    "7. Hyperparameter Tuning:\n",
    "Tuning Complexity: Some regularization techniques introduce hyperparameters (e.g., dropout rate, regularization strength). Consider the effort required for hyperparameter tuning.\n",
    "Sensitivity to Hyperparameters: Sensitivity to hyperparameters varies; dropout rate, for instance, might need careful tuning for optimal performance.\n",
    "8. Combinations of Regularization Techniques:\n",
    "Synergies and Conflicts: Different regularization techniques can be combined, but their interactions should be considered. For example, dropout and batch normalization might not always work well together.\n",
    "9. Availability in Frameworks:\n",
    "Implementation in Frameworks: Ensure that the chosen regularization technique is supported and well-implemented in the deep learning framework you are using.\n",
    "10. Empirical Testing:\n",
    "Experimentation: Conduct empirical tests on a validation set to observe the impact of different regularization techniques. Consider variations in model performance across different tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
